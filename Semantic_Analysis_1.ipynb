{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 2\n",
        "- 1. 패턴 기반 자연어 처리\n",
        "- 2. Bag-of-Words(BoW)\n",
        "- 3. NLP 파이프 라인\n",
        "- 4. 자연어 처리의 어려움\n",
        "- 5. 자연어 처리의 대표적 패러다임"
      ],
      "metadata": {
        "id": "ymWDqCFSI5Ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 패턴 기반 자연어 처리\n",
        "- 자연어 처리란 컴퓨터가 자연어를 기반으로 이 세상에 관해 무언가를 배우고 이해할 수 있도록 자연어를 변환하고 처리하여 활용하는 기술 및 분야를 말함.\n",
        "- 초창기 자연어 처리 -> 문자열 매칭을 통해 판단하는 패턴 기반 NLP\n",
        "   - 패턴, 규칙들을 정규 표현식으로 나타냄.\n",
        "   - 그러나 컼퓨터가 자연어의 의미를 파악하도록 하는데는 무리.\n",
        "   - 정규 표현식을 활용하면 간단한 문자열들은 매칭 가능.\n",
        "- 문자열의 의미를 기반으로 매칭 혹은 해석이 가능해야 함.\n",
        "- 분석 결과를 벡터로 나타낼 수 있다면 벡터 공간에서의 연산을 통해 다양한 작업을 수행할 수도 있으며 기계학습을 활용하기에도 좋음.\n",
        "\n",
        "# 2. Bag-of-Words\n",
        "- 주어진 문장 혹은 문서의 의미를 벡터로 나타내기 위해 유의미한 단어들의 등장 횟수를 벡터 내의 각 자릿수로 활용하는 방식\n",
        "- 각 토큰의 등장 횟수(보통 대소문자 구분 안함.)\n",
        "- 유의미한 단어들의 등장 횟수를 카운트해서 벡터로 변환함.\n",
        "- Bag-of-Words의 단점\n",
        "  - 1. 문서, 문장이 길어지면 성능이 저하됨.\n",
        "  - 2. 단어 수가 많으면 용량이 커져 비효율적임.\n",
        "  - 3. 단어들의 순서 및 문법 정보를 손상시킴.\n",
        "  - 4. 주변 문맥 관여 X\n",
        "\n",
        "# 3.NLP 파이프 라인\n",
        "- NL Understanding\n",
        "- NL Generation\n",
        "- 일반적인 자연어 처리 및 응답에 적용되는 전형적인 파이프 라인\n",
        "  - 1. 파싱: 특징 및 수치 자료 추출\n",
        "  - 2. 분석: 특징 추가 생성 및 결합\n",
        "  - 3. 생성: (필요시) 응답문 작성\n",
        "  - 4. 실행: 대화 내역 및 챗봇의 목표에 기초하여 최종 응답 처리 혹은 관련 프로그램 등 실행\n",
        "\n",
        "# 4. 자연어 처리의 어려움\n",
        "- 언어의 중의성\n",
        "- 규칙의 예외\n",
        "- 언어의 유연성 및 확장성\n",
        "\n",
        "# 5.자연어 처리의 대표적 패러다임\n",
        "- 규칙 기반 접근\n",
        "- 통계 기반 접근\n",
        "- 머신러닝/딥러닝 기반 접근"
      ],
      "metadata": {
        "id": "XLNti1R8J7Ea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 3\n",
        "- 1. 언어학 기초\n",
        "- 2. 간단한 토큰 생성기\n",
        "- 3. 내적 기반의 유사도\n",
        "- 4. 한국어 형태소 분석기"
      ],
      "metadata": {
        "id": "NE4WQTrMQCzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 언어학 기초\n",
        "- 음절(Syllable)\n",
        "- 형태소(Morpheme)\n",
        "- 어절(Word Pharse)\n",
        "\n",
        "# 2. 간단한 토큰 생성기\n",
        "- 텍스트 정보를 분석하기 위한 최소 단위\n",
        "  - 문자 단위(char-level)\n",
        "  - 단어 단위(word-level)\n",
        "- 영어, 한글: 단위 단위가 되어서야 의미를 가지므로 word-level 방식이 선호됨.\n",
        "- 토큰(token): 자연어 처리에서 집중하고 최소 의미 단위를 뜻하는 단어로서 어절, 형태소 등의 단위로 구분 가능함.\n",
        "- 토큰화(tokenization): 주어진 문장, 문서 등 raw 자연어 입력을 토큰 단위로 변경해주는 작업을 의미하며, 토큰화를 담당하는 객체 혹은 모듈은 tokenizer라고 부르기도 함.\n",
        "\n",
        "- 단어 토큰화 및 벡터화\n",
        "  - 1. 문장 -> split -> 어절 단위로 분리\n",
        "  - 2. 토큰들(중복 허용) 중 set을 사용하여 토큰 중복 제거 -> Vocab 생성\n",
        "  - 3. 벡터화: 생성된 Vocab 및 입력 문장을 바탕으로 입력 문장을 one-hot vector로 나타냄.\n",
        "\n",
        "- one-hot encoding: 다중 클래스 분류 문제에서 label을 만들던 방식과 유사함.\n",
        "  - 1. 말뭉치 내의 모든 단어에 대해 중복을 제거한 리스트를 만듦.\n",
        "  - 2. 모든 단어 수에 해당하는 크기를 갖도록 feature vector 크기를 설정함.\n",
        "  - 3. 각 단어의 feature vector는 리스트 내 해당 단어의 index와 같은 위치의 feature는 1, 나머지 feature들은 전부 0으로 설정함.\n",
        "- one-hot encoding을 통해 나온 feature vector는 표현하고자 하는 단어 인덱스에 해당하는 값만 1이고 나머지는 전부 0의 값을 가짐.\n",
        "- one-hot encoding 장점: 직관적이고 효과적인 벡터화 방식 중 하나임.\n",
        "- one-hot encoding 단점: 단어 수가 늘어남에 따라 벡터의 크기가 같이 늘어나야 하므로 메모리 비효율이 굉장히 심함.\n",
        "- one-hot encoding 단점 해결책: Bag-of-Words -> 벡터의 길이는 토큰 수와 같다는 점은 원-핫 벡터와 유사하나 이러한 벡터가 단어 하나를 나타내는 것이 아니라 문장이나 문서를 나타내게 되어 메모리 효율성이 훨씬 개선됨.\n",
        "\n",
        "- 말뭉치(corpus): NLP 분야에서는 모델 학습에 사용하는 데이터 셋\n",
        "  - corpus 정의: 자연어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합\n",
        "\n",
        "# 3. 내적 기반의 유사도\n",
        "- Bag-of-Words 방식으로 나타내어진 문장이나 문서의 유사성을 비교하고 싶다면, 벡터 내적값을 구하면 됨.\n",
        "- 내적값이 높을수록 겹치는 단어가 많은 문장 혹은 문서라고 볼 수 있다.\n",
        "\n",
        "# 4. 한국어 형태소 분석기\n",
        "- 고립어: 중국어\n",
        "- 굴절어: 영어, 독일어\n",
        "- 교착어: 한글, 일본어\n",
        "\n",
        "- Okt\n",
        "- Kkma\n",
        "- Mecab\n",
        "  - moprhs 메서드: 토큰화\n",
        "  - pos 메서드: lexicon\n"
      ],
      "metadata": {
        "id": "2HJkc6ZDQWEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 4\n",
        "- 1. n-gram\n",
        "- 2. 불용어 제거\n",
        "- 3. 어휘 정규화\n",
        "- 4. 감정 분석 예시"
      ],
      "metadata": {
        "id": "bQf8NLW0b9q8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. n-gram\n",
        "- 최대 n개의 토큰들로 이루어진 순차열을 의미하며, 여러 토큰들로 합성된 의미를 단일 토큰처럼 나타내기 위해 쓰임.\n",
        "- n-gram 장점: 유니그램 토큰으로는 나타내기 힘든 표현들을 관리할 수 있다.\n",
        "- n-gram 단점: 'at the'와 같이 굉장히 자주 등장하면서 크게 의미있지 않은 n-gram도 추가될 수 있다는 것은 Vocab 관리 면에서 단점으로 작용한다.\n",
        "\n",
        "# 2. 불용어 제거\n",
        "- 자주 출현하지만 그 의미는 중요치 않은 단어들을 의미함.\n",
        "\n",
        "# 3. 어휘 정규화\n",
        "- 실질적으로 의미가 같으나 표현 방벙이 다른 단어들을 통합하는 과정\n",
        "  - 1. 대소문자 통합\n",
        "    - 기계학습 모델의 과적합을 줄이는데 도움이 되며, 검색에 활용될 경우 검색 엔진의 재현율 개선에 기여할 수 있음.\n",
        "    \n",
        "    - recall = (Number of relevant documents retrieved / Total number of relevant documents) * 100\n",
        "    \n",
        "    (검색된 관련 문서 수 / 전체 관련 문서 수) * 100\n",
        "    \n",
        "    - precision = (Number of relevant documents retrieved / Total number of documents retrieved) * 100\n",
        "\n",
        "    (검색된 관련 문서 수 / 검색된 전체 문서 수) * 100\n",
        "\n",
        "    - 대소문자 통합이 검색의 재현율을 높이지만 정밀도를 낮추게 됨.\n",
        "    - 사용자가 목표로 하지 않는 다른 결과들도 검색 결과에 포함될 가능성이 높아짐.\n",
        "  \n",
        "  - 2. 어간 추출(stemming)\n",
        "    - 단어 끝의 다양한 접미사들에 의한 의미 차이를 제거하여 핵심만 남기는 정규화 방식\n",
        "    - 재현율을 높이는 데는 도움이 되지만 정밀도를 크게 감소시킬 수도 있음.\n",
        "    - 어간 추출 규칙\n",
        "      - 1. 단어가 둘 이상의 s로 끝나면, 어간은 그 단어 자체이며 별도의 접미사 없음.\n",
        "      - 2. 단어가 하나의 s로 끝나면, 어간은 s를 제외한 부분이며, s가 접미사가 됨.\n",
        "      - 3. 단어가 s로 끝나지 않으면, 어간은 그 단어 자체이고 별도의 접미사 없음.\n",
        "\n",
        "  - 3. 표제어 추출(lemmatization)\n",
        "    - 토큰들을 해당 의미의 근본적인 형태인 어근 수준으로 정규화하는 방식임.\n",
        "    - 형태적인 부분보다 의미적인 부분에 집중하여 정규화를 수행함.\n",
        "\n",
        "# 4. 감정 분석 예시\n",
        "- 단어 조합이나 문구, 문장 등에 담긴 감정을 분류하고 측정하는 작업을 의미함.\n",
        "  - 1. 규칙 기반\n",
        "  - 2. 기계학습 기반"
      ],
      "metadata": {
        "id": "dboJJsv0cMg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 5\n",
        "- 1. 단어 출현 빈도(TF)\n",
        "- 2. 벡터 표현과 유사도 계산\n",
        "- 3. 역문서 빈도(IDF)\n",
        "- 4. TF-IDF"
      ],
      "metadata": {
        "id": "AkYKoRAfk2R6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 단어 출현 빈도(TF)\n",
        "  - bag-of-words를 전체 문장, 문서 길이에 대해 정규화하면, 단어 출현 횟수가 아닌 단어 출현 빈도 or 비율이 된며 더욱 우수한 벡터 표현이 된다고 할 수 있음.\n",
        "  - 특정 단어의 출현 빈도\n",
        "\n",
        "# 2. 벡터 표현과 유사도 계산\n",
        "  - 영벡터를 복사한 뒤, 단어 출현 빈도에 대한 벡터 생성\n",
        "  - 단어 출현 빈도는 각 단어의 출현 횟수를 해당 문서의 길이로 나누어주어야 함.\n",
        "  - 유사도 계산\n",
        "    - 코사인 유사도 주로 활용\n",
        "    - 단어들 사이의 TF비율에 근거해서 유사도를 판단.\n",
        "    - 코사인 유사도 값이 높을수록 유사도가 높음.\n",
        "\n",
        "# 3. 역문서 빈도(IDF)\n",
        "  - TF만으로는 부족해\n",
        "  - 각 단어에 대해 해당 단어가 출현한 문서 수를 전체 문서 수에 대해 나눈 것으로서, 많은 문서에서 등장하는 단어일수록 IDF 값이 낮아짐.\n",
        "  - IDF 값이 높을수록 특정 문서에서만 등장하는 단어가 되며, 이는 각 문서의 주제를 대표하는 단어일 확률이 높아짐을 의미함.\n",
        "\n",
        "# 4. TF-IDF\n",
        "  - 기본적인 개념은 TF와 IDF를 곱한 특징점으로서 단어의 출현 빈도와 특정 문서에서의 희귀도를 모두 고려한 NLP에서의 특징점이라고 할 수 있음."
      ],
      "metadata": {
        "id": "kgDPi4zKlC9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 6\n",
        "- 1. TF-IDF\n",
        "- 2. TF-IDF의 파생 형태\n",
        "- 3. BM25\n"
      ],
      "metadata": {
        "id": "TMRUFt0goXZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. TF-IDF\n",
        "  - TF = 단어 출현 횟수(BoW) / 전체 토큰 수\n",
        "  - IDF = 전체 문서 수 / 해당 단어 출현 문서 수\n",
        "    - IDF 값이 높을수록 특정 문서에서만 등장하는 단어가 되며, 이는 각 문서의 주제를 대표하는 단어일 확률이 높아짐을 의미함.\n",
        "  - TF-IDF = TF * IDF\n",
        "  - 단어의 출현 빈도와 특정 문서에서의 희귀도를 모두 고려한 NLP에서의 특징점이라고 할 수 있음.\n",
        "  "
      ],
      "metadata": {
        "id": "-lx-HTgsooi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 7\n",
        "- 1. TF-IDF의 한계\n",
        "- 2. 주제 벡터\n",
        "- 3. 잠재 의미 분석 개요\n",
        "- 4. 선형 판별 분석(LDA)"
      ],
      "metadata": {
        "id": "emXGUjvx3CU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. TF-IDF의 한계(문제 발생)\n",
        "- TF-IDF 벡터는 문서에서 특정 단어들이 얼마나 중요한지 추정하는데 큰 도움이 됨.\n",
        "- n-gram에 대해서도 TF-IDF를 계산 가능함.\n",
        "- (한계) TF-IDF 벡터의 경우, 유의어들이 서로 다른 토큰일 경우, 아예 다른 벡터 성분으로 표현됨.\n",
        "- (문제 발생 1) 같은 주제를 다루지만, 구성하는 단어가 다른 두 문장, 문서의 경우 TF-IDF 벡터 공간에서 가깝지 않을 확률이 높음. -> 어휘 정규화 사용\n",
        "- 어간 추출 및 표제어 추출은 이를 좀 더 보완할 수 있으나, 이 역시 완벽하지 않고, 토큰 단계에서 유의어 등을 통합 처리하지 못하면 벡터상에서 다른 차원으로 갈라지게 됨.\n",
        "- (문제 발생 2) TF-IDF 벡터를 더하거나 빼면 문서의 단어 빈도에 대한 정보가 나오며 곱하더라도 각 차원을 나타내는 단어 간의 상관관계에 대한 정보만 나옴.\n",
        "- 단어의 의미에 대한 정보를 추출할 수 없다.(TF-IDF에서는 단어 벡터를 못 만듦.)\n",
        "- (문제 해결 단서) 단어의 의미를 벡터로 표현할 수 있다면, 이를 기반으로 문서/문장의 벡터도 표현 가능하고 단어 벡터끼리의 연산도 가능하는 등 TF-IDF 벡터를 사용할 때 보다 더 많은 가능성이 열림.\n",
        "- 단어 벡터를 나타내기 위해 각 행은 단어를, 각 열은 특정 주제나 속성을 나타내는 행렬을 단어-주제 행렬이라고 하며(반대도 가능), TF-IDF만으로는 이러한 행렬을 구할 수 없음.\n",
        "\n",
        "# 2. 주제 벡터(문제 해결 아이디어)\n",
        "- TF-IDF의 한계를 극복하기 위해 나타난 대안.\n",
        "- 각 단어별로 각 주제에 대해 어느 정도의 속성을 갖고 있는지를 나타냄.\n",
        "- 각 단어의 tf-idf 값에 해당 단어와 주제의 연관도를 곱해서 모두 취합함.\n",
        "- 실제 주제 벡터(가중치)를 학습을 통해 정하게 됨.\n",
        "- 주제 벡터 == 주제 가중치 * TF-IDF(or BoW)\n",
        "- 실제 의미 분석에서는 선형대수 및 기계학습을 활용하여 연관도를 계산함.\n",
        "- 주제-단어 벡터(전치된 형태도 가능)를 얻는 것이 일반적인 의미 분석의 핵심.\n",
        "\n",
        "\n",
        "                 cat | dog | apple | lion | NYC  | love\n",
        "    petness      0.3 | 0.3 | 0.0   | 0.0  | -0.2 | 0.2\n",
        "\n",
        "    animalness   0.1 | 0.1 | -0.1  | 0.5  | 0.1  | -0.1\n",
        "\n",
        "    cityness     0.0 | -0.1| 0.2   | -0.1 | 0.5  | 0.1\n",
        "\n",
        "- 위와 같은 벡터를 구하는 것이 핵심이다. 이같은 주제 가중치 벡터를 어떻게 뽑아낼 것인가?\n",
        "\n",
        "\n",
        "# 3. 잠재 의미 분석(Latent Semantic Analysis, LSA) 개요\n",
        "- 주제 가중치를 구하기 위해서 쓰는 방법 중 하나이다.\n",
        "- 알고리즘적으로 단어의 의미를 파악하여 주제와의 연관도를 매길 수 있어야 함.\n",
        "- 단어를 알려면 그 일행을 봐야 함. -> 워드 임베딩 기술 발전\n",
        "- 어떤 단어의 의미는 같이 출현하는 단어들로부터 추론할 수 있음.\n",
        "- BoW 및 TF-IDF에서는 이미 같이 출현하는 단어들의 상대적인 빈도 정보를 포함하고 있음.\n",
        "- 의미 분석에서는 여러 문서들의 TF-IDF 벡터를 모은 문서-벡터 행렬을 기반으로 단어들의 의미를 분석함.\n",
        "- 의미 분석에서 가장 핵심이 되는 기법으로 단어의 숨은 의미를 파악한다는 뜻의 잠재 의미 분석(LSA)가 있음.\n",
        "- LSA는 TF-IDF 행렬(또는 BoW 행렬)을 분석해서 단어들을 주제들로 요약하는 알고리즘의 일종\n",
        "  - LSA의 개념을 일부 공유하나 더욱 간단한 기법으로 LDA(Linear Discriminant Analysis, 선형 판별 분석)이 있음.\n",
        "\n",
        "# 4. LDA(선형 판별 분석) 분류기\n",
        "- LDA 분류 과정\n",
        "  - 1. 토큰화\n",
        "  - 2. TF-IDF 벡터 구하기\n",
        "  - 3. 각 class의 TF-IDF 벡터 무게중심(벡터 평균)을 계산함.\n",
        "  - 4. 두 무게중심을 잇는 직선을 나타내는 벡터를 구한다.\n",
        "  - 5. 각 TF-IDF 벡터들을 무게중심을 이은 벡터에 투영하여 길이를 계산 후, 어느 class에 속하는지 판별한다.\n",
        "\n",
        "- Linear Discriminant Analysis에서는 class 정보에 기반하여 벡터 공간 상에서 하나의 주제에 대한 벡터를 도출하고 그에 기반하여 주제에 대한 연관도를 각 단어 및 문서에서 계산하는 것이라고 볼 수 있음.\n",
        "\n",
        "- LSA는 하나의 주제가 아니라 우리가 원하는 수의 주제에 대해 벡터 공간 상에서 주제별 벡터를 도출하고, 이를 바탕으로 각 단어와 문서를 주제 벡터로 표현하는 기법\n",
        "  - LSA는 기본적으로 SVD(특이값 분해)가 활용되며, PCA를 이용한 의미 분석도 가능함."
      ],
      "metadata": {
        "id": "rJKRQGgG3RJT"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}