{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "eSwMPrnDhKjI"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "sentence = \"Thomas Jefferson began building Monticello at the age of 26.\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokens = tokenizer.tokenize(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4jjhApkV8Bu",
        "outputId": "e1687f3c-b28a-4454-c992-16a1b4b46f04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Thomas', 'Jefferson', 'began', 'building', 'Monticello', 'at', 'the', 'age', 'of', '26', '.']\n"
          ]
        }
      ],
      "source": [
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5eaaEyCWjW8"
      },
      "source": [
        "#n-gram\n",
        "- 최대 n개의 토큰들로 이루어진 순차열을 의미하며, 여러 토큰들로 합성된 의미를 단일 토큰처럼 나타내기 위해 쓰임.\n",
        "- 'ice cream'이라는 단어는 단순 토크나이저로 분리하면, 'ice', 'cream'이 분리되는데 2-gram을 활용하면 'ice cream'이 한 토큰으로 처리됨.\n",
        "- n-gram에서는 각 토큰이 튜플 형태와 유사하게 구성된다고 생각할 수 있지만, 실제로는 vocab에서 관리할 때는 튜플 형태가 아니라 ' '단위로 이어붙여서 주로 활용함.\n",
        "- 'ice', 'cream' -> 'ice cream'\n",
        "- n-gram의 장점: n-gram을 통해 유니그램 토큰으로는 나타내기 힘든 표현들을 관리할 수 있는 점이다.\n",
        "- n-gram의 단점: 'at the'와 같이 불용어가 자주 등장하면서 크게 의미가 있지 않은 n-gram도 추가될 수 있다는 것은 vocab 관리 면에서 단점으로 작용할 수 있다.\n",
        "\n",
        "#불용어\n",
        "- 자주 출현하지만 그 의미는 중요치 않은 단어들을 의미함.\n",
        "- a, an, the, and, or 등등...\n",
        "- n-gram에서는 불용어 제거가 더욱 중요함.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zWALSQUcV-00"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPT0dXpUWN3Z",
        "outputId": "bbe0afc3-f3df-4f3c-b281-b335ff3e16fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Thomas', 'Jefferson'),\n",
              " ('Jefferson', 'began'),\n",
              " ('began', 'building'),\n",
              " ('building', 'Monticello'),\n",
              " ('Monticello', 'at'),\n",
              " ('at', 'the'),\n",
              " ('the', 'age'),\n",
              " ('age', 'of'),\n",
              " ('of', '26'),\n",
              " ('26', '.')]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ngrams(tokens,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4G0ifVuWWWV",
        "outputId": "1e6e6970-75c4-4a3a-efca-610764bd168b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Thomas', 'Jefferson', 'began'),\n",
              " ('Jefferson', 'began', 'building'),\n",
              " ('began', 'building', 'Monticello'),\n",
              " ('building', 'Monticello', 'at'),\n",
              " ('Monticello', 'at', 'the'),\n",
              " ('at', 'the', 'age'),\n",
              " ('the', 'age', 'of'),\n",
              " ('age', 'of', '26'),\n",
              " ('of', '26', '.')]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ngrams(tokens,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w7zxOo_OWbvA"
      },
      "outputs": [],
      "source": [
        "# 불용어 제거\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "35-UPiqfabHI",
        "outputId": "765be568-70bf-4a57-e0b9-7d50e4aadc5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "318"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sklearn_stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "1iC1lneXapbw"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xSXT0Aaa4O3",
        "outputId": "68fb6d33-66ed-4700-ba34-7c34a31201b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'herself', 'your', 'was', 'had', 'are', 'a', 'above', 'yours', 'hasn', 'once', \"you'll\", 'here', \"needn't\", 'me', 'isn', \"weren't\", 'this', 'of', \"haven't\", 'over', 'himself', 'if', 'the', 'out', 'why', 'both', 'an', 'from', \"doesn't\", 'i', 'can', 'were', 'll', \"it's\", 'has', 'very', 'too', \"hadn't\", 'or', 'hadn', 'our', 'do', 'these', 'until', 'few', 'having', 'some', 's', \"isn't\", 'there', 'and', \"won't\", 'below', 'theirs', 'under', 'weren', 'them', 'than', 'which', 'same', 'm', 'before', \"don't\", \"you've\", 'does', 'she', 'should', 'mustn', 'as', 'no', \"should've\", 'we', 'when', \"wouldn't\", 'up', 'doesn', 'you', \"that'll\", 'so', 'ours', 'don', 'won', 'only', 'yourself', \"mightn't\", 'own', 'being', 'wasn', 'nor', 'd', 'ma', \"wasn't\", 'didn', 'wouldn', 'what', 'again', 'against', 'after', 'yourselves', 't', 'their', 'those', 'haven', 'between', 'couldn', 'ain', \"you'd\", 'most', 'about', 'he', \"didn't\", 'further', 'for', 'but', 'itself', 'been', 'any', 'other', 'not', 'will', 're', 'how', 'needn', 'during', 'ourselves', 'his', 'aren', 'mightn', 'it', 'down', 'to', \"mustn't\", 'each', \"she's\", 'who', 'such', 'is', 'into', 'through', 'that', 'her', 'on', 'doing', \"shouldn't\", \"aren't\", 'while', 'where', 'shan', 'now', 'shouldn', 'my', \"you're\", 'themselves', 'whom', 've', 'just', 'have', 'with', 'more', 'all', 'they', \"hasn't\", 'y', 'hers', 'off', 'its', 'because', 'in', 'o', 'am', 'him', 'by', \"shan't\", 'myself', 'then', 'at', \"couldn't\", 'be', 'did'}\n"
          ]
        }
      ],
      "source": [
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRvTH-acbWrX",
        "outputId": "e1a29a3a-f993-41af-b207-c07c37c294fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "len(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "S5Bz8029bi1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1fb004-f396-45cf-9ef5-750e1a69dec0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "378"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# nltk 불용어와 sklearn 불용어 합집합\n",
        "# 총 378개로 nltk에만 있고 scikit-learn에는 없는 불용어는 60개이다.\n",
        "len(stop_words.union(sklearn_stop_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "gBch0BVOdBKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1a09edc-9567-4a39-cfc1-cc3d8512f3d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "119"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# nltk와 scikit-learn에 모두 있는 불용어는 119개로, 합집합의 1/3에 못 미친다.\n",
        "# 즉, 라이브러리마다 불용어는 주관적임.\n",
        "len(stop_words.intersection(sklearn_stop_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#어휘 정규화(normalization)\n",
        "- 실질적으로는 의미가 같으나 표현 방법이 다른 단어들을 통합하는 과정을 의미함.\n",
        "- 대소문자 통합, 어간 추출, 표제어 추출 -> 비슷한 의미를 가진 토큰들을 통합함.\n",
        "- 대소문자를 통합하면 그러지 않을 때 보다 약 절반 정도로 토큰 수를 줄일 수 있음.\n",
        "- 그러나 대문자화(capitalization)를 통해 인명, 지명, 상표 등 고유 명사를 표기하는 경우가 많으므로 개체명 인식 등 일부 작업에서는 대소문자 통합이 성능 저하를 일으킬 수 있음.\n",
        "- (문제 발생) 파이썬에서의 str.lower() 메서드를 통한 통합은 의도적으로 쓰인 대문자와 문장 맨 앞에서 쓰이는 대문자를 구분하지 못하기 때문에, 정보 손실을 야기할 수 있음.\n",
        "- (문제 해결) 의도적으로 쓰인 대문자와 문장 맨 앞의 대문자를 구분하고, 문장 맨 앞의 대문자일 경우에만 lower()를 적용하는 방법이 있음.\n",
        "---\n",
        "- 대소문자 통합은 기계학습 모델의 과적합을 줄이는 데 도움이 되며, 검색에 활용될 경우 검색 엔진의 재현율(recall) 개선에 기여할 수 있음.\n",
        "- 만약 대소문자 통합이 되지 않은 검색 엔진으로 'Age'를 검색하면 'age'를 검색할 때와 다른 결과가 나오게 됨. 반면 대소문자 통합을 검색 색인(vocab) 및 검색어 모두에 대해 수행하면 'age'에 관한 모든 문서들이 검색됨.\n",
        "- 일반적으로 어휘 정규화는 검색의 재현율을 높이지만 정밀도를 낮추게 됨.\n",
        "- 사용자가 목표로 하지 않는 다른 결과들도 검색 결과에 포함될 가능성이 높아짐.\n",
        "- 이에 검색 엔진에서는 사용자가 정규화를 회피하는 수단으로 \"\"기능을 제공함.\n",
        "- 따옴표 안에 들어 있는 단어, 문장을 그대로 포함하는 문서를 검색함.\n",
        "---\n",
        "- 보다 적극적인 형태의 정규화로서 어간 추출(stemming)이라는 기법이 존재\n",
        "- 어간 추출(stemming): 단어 끝의 다양한 접미사들에 의한 의미 차이를 제거하여 핵심만 남기는 정규화 방식\n",
        "- ex) house, houses, housing과 같은 단어들에서 공통 어간인 house를 추출하여 전처리\n",
        "- 추출된 어간은 반드시 사전에 나오는 형태여야 할 필요는 없고, 한 어간의 다양한 형태를 대표할 수 있으면 됨.\n",
        "- stemming 또한 정규화의 일종이므로 재현율을 높이는 데는 도움이 되지만, 정밀도를 크게 감소시킬 수도 있음.\n",
        "- \"dr house call\"이 아니라 \"Dr.House's call\"을 검색하고 싶다면 stemming을 하지 않는 것이 좋음.\n",
        "\n",
        "\n",
        "# 검색에서의 재현율(recall) 및 정밀도(precision) 비교\n",
        "- 재현율(recall)\n",
        "  - 관심있는 어떤 결과가 얼마나 포함되어 있는냐를 나타내는 것임.\n",
        "  - recall = (Number of relevant documents retrieved / Total number of relevant documents) * 100\n",
        "- 정밀도(precision)\n",
        "  - 모델의 결과 중 관심있는 결과의 비율\n",
        "  - precision = (Number of relevant documents retrieved / Total number of documents retrieved) * 100\n",
        "- 재현율과 정밀도는 반비례 관계의 성질을 띤다.\n",
        "\n",
        "# stemming\n",
        "- 어간 추출 규칙\n",
        "  - 1. 단어가 둘 이상의 s로 끝나면, 어간은 그 단어 자체이며 별도의 접미사 없음.\n",
        "  - 2. 단어가 하나의 s로 끝나면 어간은 s를 제외한 부분이며, s가 접미사 됨.\n",
        "  - 3. 단어가 s로 끝나지 않으면 어간은 그 단어 자체이고 별도의 접미사가 없음.\n"
      ],
      "metadata": {
        "id": "Cp5liF6NfbqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "import re\n",
        "def stem(pharse):\n",
        "  return ' '.join([re.findall('^(.*ss|.*?)(s)?$',word)[0][0].strip(\"'\") for word in pharse.lower().split()])"
      ],
      "metadata": {
        "id": "q0nT3MB3JroG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stem('houses')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uacHT6fxKYxJ",
        "outputId": "66b698c9-af79-4dab-cb55-fd374af66f1c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'house'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stem(\"Doctor House's calls\") # recall은 증가함."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1fP8WLu3KcBK",
        "outputId": "a5479c3e-66b9-46df-e496-73c72ded07f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'doctor house call'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 어간 추출 정규화 방식\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "0MGZzCqlLsob"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join([stemmer.stem(w).strip(\"'\") for w in \"dish washer's washed dishes\".split()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DT8IVDPwL_0A",
        "outputId": "c64af355-faad-41d2-e5b1-1c4c6a643fdc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dish washer wash dish'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 어휘 정규화\n",
        "- lemmatization(표제어 추출)\n",
        "  - 토큰들을 해당 의미의 근본적인 형태인 어근 수준으로 정규화하는 방식\n",
        "  - 형태적인 부분보다 의미적인 부분에 집중하여 정규화를 수행함.\n",
        "  - 고급 표제어 추출기에서는 각 토큰의 품사는 주변 문맥까지 고려하여 최종적으로 표제어 추출을 진행함.\n",
        "  - 1. 토큰 품사 요구\n",
        "  - 2. 주변 문맥에 따라 정규화\n",
        "- stemming(어간 추출)과 lemmatization(표제어 추출) 비교\n",
        "  - 어간 추출\n",
        "    - better -> \"bett\", \"bet\"와 같이 \"er\"을 제거하는데 집중\n",
        "  - 표제어 추출(사전 정보 필요)\n",
        "    - better -> \"betterment\", \"best\", \"good\"과 같이 변환"
      ],
      "metadata": {
        "id": "uEcfDfJkMdE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatization\n",
        "import nltk\n",
        "nltk.download('wordnet') # WordNet: 프린스턴 대학교에서 구축한 방대한 유의어 그래프 데이터\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNAct1piMNK5",
        "outputId": "ff1520ef-090e-4009-db2a-b45ea1908082"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"better\") # 2번째 인수를 생략하는 경우, 명사를 뜻하는 \"n\"이 적용됨."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VWXYZaa-PY2I",
        "outputId": "f40e25e6-297f-4a45-f9f3-92b92dfc7284"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'better'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"better\", pos=\"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8kWJJ172PtGJ",
        "outputId": "a99d2edb-63f2-41b0-a6e7-20086e7ea8e3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"good\", pos=\"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-tV_vkOmQZfv",
        "outputId": "e618266f-1525-4087-d365-e79fb90d8788"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"goods\", pos=\"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lao9sjiVQe6C",
        "outputId": "711e6ecb-56f7-47d5-aad5-2a7c92fd9f8e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'goods'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"good\", pos=\"n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5GJYgYKGQlLZ",
        "outputId": "ec82304e-c96a-4b4f-dd26-0e5209f22ff4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"goodness\", pos=\"n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FmL5B9FMQmYR",
        "outputId": "2c168ff9-d92e-4665-c5a1-2af0e7e53fff"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'goodness'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"best\", pos=\"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qhfkrYGqQpvD",
        "outputId": "0784bf1f-ecf5-418d-8ca0-0af1a73903fb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'best'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 감정분석(sentiment analysis)\n",
        "- 단어 조합이나 문구, 문장 등에 담긴 감정을 분류하고 측정하는 작업을 의미\n",
        "- 여기서는 1. 간단한 토큰화 및 규칙 기반, 2. 기계학습 기반으로 감정 분석이 어느 정도 가능함을 본다.\n",
        "- 감정 분석 알고리즘의 목표\n",
        "  - 주어진 입력에 대해 -1부터 +1까지 긍정적인 정도를 출력\n",
        "\n",
        "# 1. 규칙 기반 감정 분석\n",
        "  - 텍스트에서 특정 키워드들(n-gram)을 찾음. 그 후, 키워드들에 부여된 점수를 취합하는 것이 기본적인 방식\n",
        "  - (키워드, 감정 점수) 쌍들을 담은 사전(dictionary)이 필요함.\n",
        "  - 키워드를 정확히 매칭하기 위한 토큰화 방식이 중요하며, 이 때 정규화 여부 또한 중요할 수 있음.\n",
        "\n",
        "# 2. 기계학습 기반 감정 분석\n",
        "  - 문장, 문서들(데이터)과 그에 대한 점수(레이블)쌍이 필요함.\n",
        "  - 기본적으로 지도학습 형태로 감정분석에 대해 학습한 후, 추론을 수행함."
      ],
      "metadata": {
        "id": "hj6DMXztRAZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHYkbopRU5wo",
        "outputId": "4eb0ce61-d187-4559-e0a2-0dd3e465041c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VADER 규칙 기반 감정 분석기\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sa = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "swQ28FuXQuAx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sa.lexicon # SentimentIntensityAnalyzer.lexicon에는 토큰-감정 점수 쌍들이 들어 있다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPloO_WrUUPS",
        "outputId": "d9d75a07-b88a-4aef-a2d6-128d519613e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'$:': -1.5,\n",
              " '%)': -0.4,\n",
              " '%-)': -1.5,\n",
              " '&-:': -0.4,\n",
              " '&:': -0.7,\n",
              " \"( '}{' )\": 1.6,\n",
              " '(%': -0.9,\n",
              " \"('-:\": 2.2,\n",
              " \"(':\": 2.3,\n",
              " '((-:': 2.1,\n",
              " '(*': 1.1,\n",
              " '(-%': -0.7,\n",
              " '(-*': 1.3,\n",
              " '(-:': 1.6,\n",
              " '(-:0': 2.8,\n",
              " '(-:<': -0.4,\n",
              " '(-:o': 1.5,\n",
              " '(-:O': 1.5,\n",
              " '(-:{': -0.1,\n",
              " '(-:|>*': 1.9,\n",
              " '(-;': 1.3,\n",
              " '(-;|': 2.1,\n",
              " '(8': 2.6,\n",
              " '(:': 2.2,\n",
              " '(:0': 2.4,\n",
              " '(:<': -0.2,\n",
              " '(:o': 2.5,\n",
              " '(:O': 2.5,\n",
              " '(;': 1.1,\n",
              " '(;<': 0.3,\n",
              " '(=': 2.2,\n",
              " '(?:': 2.1,\n",
              " '(^:': 1.5,\n",
              " '(^;': 1.5,\n",
              " '(^;0': 2.0,\n",
              " '(^;o': 1.9,\n",
              " '(o:': 1.6,\n",
              " \")':\": -2.0,\n",
              " \")-':\": -2.1,\n",
              " ')-:': -2.1,\n",
              " ')-:<': -2.2,\n",
              " ')-:{': -2.1,\n",
              " '):': -1.8,\n",
              " '):<': -1.9,\n",
              " '):{': -2.3,\n",
              " ');<': -2.6,\n",
              " '*)': 0.6,\n",
              " '*-)': 0.3,\n",
              " '*-:': 2.1,\n",
              " '*-;': 2.4,\n",
              " '*:': 1.9,\n",
              " '*<|:-)': 1.6,\n",
              " '*\\\\0/*': 2.3,\n",
              " '*^:': 1.6,\n",
              " ',-:': 1.2,\n",
              " \"---'-;-{@\": 2.3,\n",
              " '--<--<@': 2.2,\n",
              " '.-:': -1.2,\n",
              " '..###-:': -1.7,\n",
              " '..###:': -1.9,\n",
              " '/-:': -1.3,\n",
              " '/:': -1.3,\n",
              " '/:<': -1.4,\n",
              " '/=': -0.9,\n",
              " '/^:': -1.0,\n",
              " '/o:': -1.4,\n",
              " '0-8': 0.1,\n",
              " '0-|': -1.2,\n",
              " '0:)': 1.9,\n",
              " '0:-)': 1.4,\n",
              " '0:-3': 1.5,\n",
              " '0:03': 1.9,\n",
              " '0;^)': 1.6,\n",
              " '0_o': -0.3,\n",
              " '10q': 2.1,\n",
              " '1337': 2.1,\n",
              " '143': 3.2,\n",
              " '1432': 2.6,\n",
              " '14aa41': 2.4,\n",
              " '182': -2.9,\n",
              " '187': -3.1,\n",
              " '2g2b4g': 2.8,\n",
              " '2g2bt': -0.1,\n",
              " '2qt': 2.1,\n",
              " '3:(': -2.2,\n",
              " '3:)': 0.5,\n",
              " '3:-(': -2.3,\n",
              " '3:-)': -1.4,\n",
              " '4col': -2.2,\n",
              " '4q': -3.1,\n",
              " '5fs': 1.5,\n",
              " '8)': 1.9,\n",
              " '8-d': 1.7,\n",
              " '8-o': -0.3,\n",
              " '86': -1.6,\n",
              " '8d': 2.9,\n",
              " ':###..': -2.4,\n",
              " ':$': -0.2,\n",
              " ':&': -0.6,\n",
              " \":'(\": -2.2,\n",
              " \":')\": 2.3,\n",
              " \":'-(\": -2.4,\n",
              " \":'-)\": 2.7,\n",
              " ':(': -1.9,\n",
              " ':)': 2.0,\n",
              " ':*': 2.5,\n",
              " ':-###..': -2.5,\n",
              " ':-&': -0.5,\n",
              " ':-(': -1.5,\n",
              " ':-)': 1.3,\n",
              " ':-))': 2.8,\n",
              " ':-*': 1.7,\n",
              " ':-,': 1.1,\n",
              " ':-.': -0.9,\n",
              " ':-/': -1.2,\n",
              " ':-<': -1.5,\n",
              " ':-d': 2.3,\n",
              " ':-D': 2.3,\n",
              " ':-o': 0.1,\n",
              " ':-p': 1.5,\n",
              " ':-[': -1.6,\n",
              " ':-\\\\': -0.9,\n",
              " ':-c': -1.3,\n",
              " ':-|': -0.7,\n",
              " ':-||': -2.5,\n",
              " ':-Þ': 0.9,\n",
              " ':/': -1.4,\n",
              " ':3': 2.3,\n",
              " ':<': -2.1,\n",
              " ':>': 2.1,\n",
              " ':?)': 1.3,\n",
              " ':?c': -1.6,\n",
              " ':@': -2.5,\n",
              " ':d': 2.3,\n",
              " ':D': 2.3,\n",
              " ':l': -1.7,\n",
              " ':o': -0.4,\n",
              " ':p': 1.0,\n",
              " ':s': -1.2,\n",
              " ':[': -2.0,\n",
              " ':\\\\': -1.3,\n",
              " ':]': 2.2,\n",
              " ':^)': 2.1,\n",
              " ':^*': 2.6,\n",
              " ':^/': -1.2,\n",
              " ':^\\\\': -1.0,\n",
              " ':^|': -1.0,\n",
              " ':c': -2.1,\n",
              " ':c)': 2.0,\n",
              " ':o)': 2.1,\n",
              " ':o/': -1.4,\n",
              " ':o\\\\': -1.1,\n",
              " ':o|': -0.6,\n",
              " ':P': 1.4,\n",
              " ':{': -1.9,\n",
              " ':|': -0.4,\n",
              " ':}': 2.1,\n",
              " ':Þ': 1.1,\n",
              " ';)': 0.9,\n",
              " ';-)': 1.0,\n",
              " ';-*': 2.2,\n",
              " ';-]': 0.7,\n",
              " ';d': 0.8,\n",
              " ';D': 0.8,\n",
              " ';]': 0.6,\n",
              " ';^)': 1.4,\n",
              " '</3': -3.0,\n",
              " '<3': 1.9,\n",
              " '<:': 2.1,\n",
              " '<:-|': -1.4,\n",
              " '=)': 2.2,\n",
              " '=-3': 2.0,\n",
              " '=-d': 2.4,\n",
              " '=-D': 2.4,\n",
              " '=/': -1.4,\n",
              " '=3': 2.1,\n",
              " '=d': 2.3,\n",
              " '=D': 2.3,\n",
              " '=l': -1.2,\n",
              " '=\\\\': -1.2,\n",
              " '=]': 1.6,\n",
              " '=p': 1.3,\n",
              " '=|': -0.8,\n",
              " '>-:': -2.0,\n",
              " '>.<': -1.3,\n",
              " '>:': -2.1,\n",
              " '>:(': -2.7,\n",
              " '>:)': 0.4,\n",
              " '>:-(': -2.7,\n",
              " '>:-)': -0.4,\n",
              " '>:/': -1.6,\n",
              " '>:o': -1.2,\n",
              " '>:p': 1.0,\n",
              " '>:[': -2.1,\n",
              " '>:\\\\': -1.7,\n",
              " '>;(': -2.9,\n",
              " '>;)': 0.1,\n",
              " '>_>^': 2.1,\n",
              " '@:': -2.1,\n",
              " '@>-->--': 2.1,\n",
              " \"@}-;-'---\": 2.2,\n",
              " 'aas': 2.5,\n",
              " 'aayf': 2.7,\n",
              " 'afu': -2.9,\n",
              " 'alol': 2.8,\n",
              " 'ambw': 2.9,\n",
              " 'aml': 3.4,\n",
              " 'atab': -1.9,\n",
              " 'awol': -1.3,\n",
              " 'ayc': 0.2,\n",
              " 'ayor': -1.2,\n",
              " 'aug-00': 0.3,\n",
              " 'bfd': -2.7,\n",
              " 'bfe': -2.6,\n",
              " 'bff': 2.9,\n",
              " 'bffn': 1.0,\n",
              " 'bl': 2.3,\n",
              " 'bsod': -2.2,\n",
              " 'btd': -2.1,\n",
              " 'btdt': -0.1,\n",
              " 'bz': 0.4,\n",
              " 'b^d': 2.6,\n",
              " 'cwot': -2.3,\n",
              " \"d-':\": -2.5,\n",
              " 'd8': -3.2,\n",
              " 'd:': 1.2,\n",
              " 'd:<': -3.2,\n",
              " 'd;': -2.9,\n",
              " 'd=': 1.5,\n",
              " 'doa': -2.3,\n",
              " 'dx': -3.0,\n",
              " 'ez': 1.5,\n",
              " 'fav': 2.0,\n",
              " 'fcol': -1.8,\n",
              " 'ff': 1.8,\n",
              " 'ffs': -2.8,\n",
              " 'fkm': -2.4,\n",
              " 'foaf': 1.8,\n",
              " 'ftw': 2.0,\n",
              " 'fu': -3.7,\n",
              " 'fubar': -3.0,\n",
              " 'fwb': 2.5,\n",
              " 'fyi': 0.8,\n",
              " 'fysa': 0.4,\n",
              " 'g1': 1.4,\n",
              " 'gg': 1.2,\n",
              " 'gga': 1.7,\n",
              " 'gigo': -0.6,\n",
              " 'gj': 2.0,\n",
              " 'gl': 1.3,\n",
              " 'gla': 2.5,\n",
              " 'gn': 1.2,\n",
              " 'gr8': 2.7,\n",
              " 'grrr': -0.4,\n",
              " 'gt': 1.1,\n",
              " 'h&k': 2.3,\n",
              " 'hagd': 2.2,\n",
              " 'hagn': 2.2,\n",
              " 'hago': 1.2,\n",
              " 'hak': 1.9,\n",
              " 'hand': 2.2,\n",
              " 'heart': 3.2,\n",
              " 'hearts': 3.3,\n",
              " 'hho1/2k': 1.4,\n",
              " 'hhoj': 2.0,\n",
              " 'hhok': 0.9,\n",
              " 'hugz': 2.0,\n",
              " 'hi5': 1.9,\n",
              " 'idk': -0.4,\n",
              " 'ijs': 0.7,\n",
              " 'ilu': 3.4,\n",
              " 'iluaaf': 2.7,\n",
              " 'ily': 3.4,\n",
              " 'ily2': 2.6,\n",
              " 'iou': 0.7,\n",
              " 'iyq': 2.3,\n",
              " 'j/j': 2.0,\n",
              " 'j/k': 1.6,\n",
              " 'j/p': 1.4,\n",
              " 'j/t': -0.2,\n",
              " 'j/w': 1.0,\n",
              " 'j4f': 1.4,\n",
              " 'j4g': 1.7,\n",
              " 'jho': 0.8,\n",
              " 'jhomf': 1.0,\n",
              " 'jj': 1.0,\n",
              " 'jk': 0.9,\n",
              " 'jp': 0.8,\n",
              " 'jt': 0.9,\n",
              " 'jw': 1.6,\n",
              " 'jealz': -1.2,\n",
              " 'k4y': 2.3,\n",
              " 'kfy': 2.3,\n",
              " 'kia': -3.2,\n",
              " 'kk': 1.5,\n",
              " 'kmuf': 2.2,\n",
              " 'l': 2.0,\n",
              " 'l&r': 2.2,\n",
              " 'laoj': 1.3,\n",
              " 'lmao': 2.9,\n",
              " 'lmbao': 1.8,\n",
              " 'lmfao': 2.5,\n",
              " 'lmso': 2.7,\n",
              " 'lol': 1.8,\n",
              " 'lolz': 2.7,\n",
              " 'lts': 1.6,\n",
              " 'ly': 2.6,\n",
              " 'ly4e': 2.7,\n",
              " 'lya': 3.3,\n",
              " 'lyb': 3.0,\n",
              " 'lyl': 3.1,\n",
              " 'lylab': 2.7,\n",
              " 'lylas': 2.6,\n",
              " 'lylb': 1.6,\n",
              " 'm8': 1.4,\n",
              " 'mia': -1.2,\n",
              " 'mml': 2.0,\n",
              " 'mofo': -2.4,\n",
              " 'muah': 2.3,\n",
              " 'mubar': -1.0,\n",
              " 'musm': 0.9,\n",
              " 'mwah': 2.5,\n",
              " 'n1': 1.9,\n",
              " 'nbd': 1.3,\n",
              " 'nbif': -0.5,\n",
              " 'nfc': -2.7,\n",
              " 'nfw': -2.4,\n",
              " 'nh': 2.2,\n",
              " 'nimby': -0.8,\n",
              " 'nimjd': -0.7,\n",
              " 'nimq': -0.2,\n",
              " 'nimy': -1.4,\n",
              " 'nitl': -1.5,\n",
              " 'nme': -2.1,\n",
              " 'noyb': -0.7,\n",
              " 'np': 1.4,\n",
              " 'ntmu': 1.4,\n",
              " 'o-8': -0.5,\n",
              " 'o-:': -0.3,\n",
              " 'o-|': -1.1,\n",
              " 'o.o': -0.8,\n",
              " 'O.o': -0.6,\n",
              " 'o.O': -0.6,\n",
              " 'o:': -0.2,\n",
              " 'o:)': 1.5,\n",
              " 'o:-)': 2.0,\n",
              " 'o:-3': 2.2,\n",
              " 'o:3': 2.3,\n",
              " 'o:<': -0.3,\n",
              " 'o;^)': 1.6,\n",
              " 'ok': 1.2,\n",
              " 'o_o': -0.5,\n",
              " 'O_o': -0.5,\n",
              " 'o_O': -0.5,\n",
              " 'pita': -2.4,\n",
              " 'pls': 0.3,\n",
              " 'plz': 0.3,\n",
              " 'pmbi': 0.8,\n",
              " 'pmfji': 0.3,\n",
              " 'pmji': 0.7,\n",
              " 'po': -2.6,\n",
              " 'ptl': 2.6,\n",
              " 'pu': -1.1,\n",
              " 'qq': -2.2,\n",
              " 'qt': 1.8,\n",
              " 'r&r': 2.4,\n",
              " 'rofl': 2.7,\n",
              " 'roflmao': 2.5,\n",
              " 'rotfl': 2.6,\n",
              " 'rotflmao': 2.8,\n",
              " 'rotflmfao': 2.5,\n",
              " 'rotflol': 3.0,\n",
              " 'rotgl': 2.9,\n",
              " 'rotglmao': 1.8,\n",
              " 's:': -1.1,\n",
              " 'sapfu': -1.1,\n",
              " 'sete': 2.8,\n",
              " 'sfete': 2.7,\n",
              " 'sgtm': 2.4,\n",
              " 'slap': 0.6,\n",
              " 'slaw': 2.1,\n",
              " 'smh': -1.3,\n",
              " 'snafu': -2.5,\n",
              " 'sob': -1.0,\n",
              " 'swak': 2.3,\n",
              " 'tgif': 2.3,\n",
              " 'thks': 1.4,\n",
              " 'thx': 1.5,\n",
              " 'tia': 2.3,\n",
              " 'tmi': -0.3,\n",
              " 'tnx': 1.1,\n",
              " 'true': 1.8,\n",
              " 'tx': 1.5,\n",
              " 'txs': 1.1,\n",
              " 'ty': 1.6,\n",
              " 'tyvm': 2.5,\n",
              " 'urw': 1.9,\n",
              " 'vbg': 2.1,\n",
              " 'vbs': 3.1,\n",
              " 'vip': 2.3,\n",
              " 'vwd': 2.6,\n",
              " 'vwp': 2.1,\n",
              " 'wag': -0.2,\n",
              " 'wd': 2.7,\n",
              " 'wilco': 0.9,\n",
              " 'wp': 1.0,\n",
              " 'wtf': -2.8,\n",
              " 'wtg': 2.1,\n",
              " 'wth': -2.4,\n",
              " 'x-d': 2.6,\n",
              " 'x-p': 1.7,\n",
              " 'xd': 2.8,\n",
              " 'xlnt': 3.0,\n",
              " 'xoxo': 3.0,\n",
              " 'xoxozzz': 2.3,\n",
              " 'xp': 1.6,\n",
              " 'xqzt': 1.6,\n",
              " 'xtc': 0.8,\n",
              " 'yolo': 1.1,\n",
              " 'yoyo': 0.4,\n",
              " 'yvw': 1.6,\n",
              " 'yw': 1.8,\n",
              " 'ywia': 2.5,\n",
              " 'zzz': -1.2,\n",
              " '[-;': 0.5,\n",
              " '[:': 1.3,\n",
              " '[;': 1.0,\n",
              " '[=': 1.7,\n",
              " '\\\\-:': -1.0,\n",
              " '\\\\:': -1.0,\n",
              " '\\\\:<': -1.7,\n",
              " '\\\\=': -1.1,\n",
              " '\\\\^:': -1.3,\n",
              " '\\\\o/': 2.2,\n",
              " '\\\\o:': -1.2,\n",
              " ']-:': -2.1,\n",
              " ']:': -1.6,\n",
              " ']:<': -2.5,\n",
              " '^<_<': 1.4,\n",
              " '^urs': -2.8,\n",
              " 'abandon': -1.9,\n",
              " 'abandoned': -2.0,\n",
              " 'abandoner': -1.9,\n",
              " 'abandoners': -1.9,\n",
              " 'abandoning': -1.6,\n",
              " 'abandonment': -2.4,\n",
              " 'abandonments': -1.7,\n",
              " 'abandons': -1.3,\n",
              " 'abducted': -2.3,\n",
              " 'abduction': -2.8,\n",
              " 'abductions': -2.0,\n",
              " 'abhor': -2.0,\n",
              " 'abhorred': -2.4,\n",
              " 'abhorrent': -3.1,\n",
              " 'abhors': -2.9,\n",
              " 'abilities': 1.0,\n",
              " 'ability': 1.3,\n",
              " 'aboard': 0.1,\n",
              " 'absentee': -1.1,\n",
              " 'absentees': -0.8,\n",
              " 'absolve': 1.2,\n",
              " 'absolved': 1.5,\n",
              " 'absolves': 1.3,\n",
              " 'absolving': 1.6,\n",
              " 'abuse': -3.2,\n",
              " 'abused': -2.3,\n",
              " 'abuser': -2.6,\n",
              " 'abusers': -2.6,\n",
              " 'abuses': -2.6,\n",
              " 'abusing': -2.0,\n",
              " 'abusive': -3.2,\n",
              " 'abusively': -2.8,\n",
              " 'abusiveness': -2.5,\n",
              " 'abusivenesses': -3.0,\n",
              " 'accept': 1.6,\n",
              " 'acceptabilities': 1.6,\n",
              " 'acceptability': 1.1,\n",
              " 'acceptable': 1.3,\n",
              " 'acceptableness': 1.3,\n",
              " 'acceptably': 1.5,\n",
              " 'acceptance': 2.0,\n",
              " 'acceptances': 1.7,\n",
              " 'acceptant': 1.6,\n",
              " 'acceptation': 1.3,\n",
              " 'acceptations': 0.9,\n",
              " 'accepted': 1.1,\n",
              " 'accepting': 1.6,\n",
              " 'accepts': 1.3,\n",
              " 'accident': -2.1,\n",
              " 'accidental': -0.3,\n",
              " 'accidentally': -1.4,\n",
              " 'accidents': -1.3,\n",
              " 'accomplish': 1.8,\n",
              " 'accomplished': 1.9,\n",
              " 'accomplishes': 1.7,\n",
              " 'accusation': -1.0,\n",
              " 'accusations': -1.3,\n",
              " 'accuse': -0.8,\n",
              " 'accused': -1.2,\n",
              " 'accuses': -1.4,\n",
              " 'accusing': -0.7,\n",
              " 'ache': -1.6,\n",
              " 'ached': -1.6,\n",
              " 'aches': -1.0,\n",
              " 'achievable': 1.3,\n",
              " 'aching': -2.2,\n",
              " 'acquit': 0.8,\n",
              " 'acquits': 0.1,\n",
              " 'acquitted': 1.0,\n",
              " 'acquitting': 1.3,\n",
              " 'acrimonious': -1.7,\n",
              " 'active': 1.7,\n",
              " 'actively': 1.3,\n",
              " 'activeness': 0.6,\n",
              " 'activenesses': 0.8,\n",
              " 'actives': 1.1,\n",
              " 'adequate': 0.9,\n",
              " 'admirability': 2.4,\n",
              " 'admirable': 2.6,\n",
              " 'admirableness': 2.2,\n",
              " 'admirably': 2.5,\n",
              " 'admiral': 1.3,\n",
              " 'admirals': 1.5,\n",
              " 'admiralties': 1.6,\n",
              " 'admiralty': 1.2,\n",
              " 'admiration': 2.5,\n",
              " 'admirations': 1.6,\n",
              " 'admire': 2.1,\n",
              " 'admired': 2.3,\n",
              " 'admirer': 1.8,\n",
              " 'admirers': 1.7,\n",
              " 'admires': 1.5,\n",
              " 'admiring': 1.6,\n",
              " 'admiringly': 2.3,\n",
              " 'admit': 0.8,\n",
              " 'admits': 1.2,\n",
              " 'admitted': 0.4,\n",
              " 'admonished': -1.9,\n",
              " 'adopt': 0.7,\n",
              " 'adopts': 0.7,\n",
              " 'adorability': 2.2,\n",
              " 'adorable': 2.2,\n",
              " 'adorableness': 2.5,\n",
              " 'adorably': 2.1,\n",
              " 'adoration': 2.9,\n",
              " 'adorations': 2.2,\n",
              " 'adore': 2.6,\n",
              " 'adored': 1.8,\n",
              " 'adorer': 1.7,\n",
              " 'adorers': 2.1,\n",
              " 'adores': 1.6,\n",
              " 'adoring': 2.6,\n",
              " 'adoringly': 2.4,\n",
              " 'adorn': 0.9,\n",
              " 'adorned': 0.8,\n",
              " 'adorner': 1.3,\n",
              " 'adorners': 0.9,\n",
              " 'adorning': 1.0,\n",
              " 'adornment': 1.3,\n",
              " 'adornments': 0.8,\n",
              " 'adorns': 0.5,\n",
              " 'advanced': 1.0,\n",
              " 'advantage': 1.0,\n",
              " 'advantaged': 1.4,\n",
              " 'advantageous': 1.5,\n",
              " 'advantageously': 1.9,\n",
              " 'advantageousness': 1.6,\n",
              " 'advantages': 1.5,\n",
              " 'advantaging': 1.6,\n",
              " 'adventure': 1.3,\n",
              " 'adventured': 1.3,\n",
              " 'adventurer': 1.2,\n",
              " 'adventurers': 0.9,\n",
              " 'adventures': 1.4,\n",
              " 'adventuresome': 1.7,\n",
              " 'adventuresomeness': 1.3,\n",
              " 'adventuress': 0.8,\n",
              " 'adventuresses': 1.4,\n",
              " 'adventuring': 2.3,\n",
              " 'adventurism': 1.5,\n",
              " 'adventurist': 1.4,\n",
              " 'adventuristic': 1.7,\n",
              " 'adventurists': 1.2,\n",
              " 'adventurous': 1.4,\n",
              " 'adventurously': 1.3,\n",
              " 'adventurousness': 1.8,\n",
              " 'adversarial': -1.5,\n",
              " 'adversaries': -1.0,\n",
              " 'adversary': -0.8,\n",
              " 'adversative': -1.2,\n",
              " 'adversatively': -0.1,\n",
              " 'adversatives': -1.0,\n",
              " 'adverse': -1.5,\n",
              " 'adversely': -0.8,\n",
              " 'adverseness': -0.6,\n",
              " 'adversities': -1.5,\n",
              " 'adversity': -1.8,\n",
              " 'affected': -0.6,\n",
              " 'affection': 2.4,\n",
              " 'affectional': 1.9,\n",
              " 'affectionally': 1.5,\n",
              " 'affectionate': 1.9,\n",
              " 'affectionately': 2.2,\n",
              " 'affectioned': 1.8,\n",
              " 'affectionless': -2.0,\n",
              " 'affections': 1.5,\n",
              " 'afflicted': -1.5,\n",
              " 'affronted': 0.2,\n",
              " 'aggravate': -2.5,\n",
              " 'aggravated': -1.9,\n",
              " 'aggravates': -1.9,\n",
              " 'aggravating': -1.2,\n",
              " 'aggress': -1.3,\n",
              " 'aggressed': -1.4,\n",
              " 'aggresses': -0.5,\n",
              " 'aggressing': -0.6,\n",
              " 'aggression': -1.2,\n",
              " 'aggressions': -1.3,\n",
              " 'aggressive': -0.6,\n",
              " 'aggressively': -1.3,\n",
              " 'aggressiveness': -1.8,\n",
              " 'aggressivities': -1.4,\n",
              " 'aggressivity': -0.6,\n",
              " 'aggressor': -0.8,\n",
              " 'aggressors': -0.9,\n",
              " 'aghast': -1.9,\n",
              " 'agitate': -1.7,\n",
              " 'agitated': -2.0,\n",
              " 'agitatedly': -1.6,\n",
              " 'agitates': -1.4,\n",
              " 'agitating': -1.8,\n",
              " 'agitation': -1.0,\n",
              " 'agitational': -1.2,\n",
              " 'agitations': -1.3,\n",
              " 'agitative': -1.3,\n",
              " 'agitato': -0.1,\n",
              " 'agitator': -1.4,\n",
              " 'agitators': -2.1,\n",
              " 'agog': 1.9,\n",
              " 'agonise': -2.1,\n",
              " 'agonised': -2.3,\n",
              " 'agonises': -2.4,\n",
              " 'agonising': -1.5,\n",
              " 'agonize': -2.3,\n",
              " 'agonized': -2.2,\n",
              " 'agonizes': -2.3,\n",
              " 'agonizing': -2.7,\n",
              " 'agonizingly': -2.3,\n",
              " 'agony': -1.8,\n",
              " 'agree': 1.5,\n",
              " 'agreeability': 1.9,\n",
              " 'agreeable': 1.8,\n",
              " 'agreeableness': 1.8,\n",
              " 'agreeablenesses': 1.3,\n",
              " 'agreeably': 1.6,\n",
              " 'agreed': 1.1,\n",
              " 'agreeing': 1.4,\n",
              " 'agreement': 2.2,\n",
              " 'agreements': 1.1,\n",
              " 'agrees': 0.8,\n",
              " 'alarm': -1.4,\n",
              " 'alarmed': -1.4,\n",
              " 'alarming': -0.5,\n",
              " 'alarmingly': -2.6,\n",
              " 'alarmism': -0.3,\n",
              " 'alarmists': -1.1,\n",
              " 'alarms': -1.1,\n",
              " 'alas': -1.1,\n",
              " 'alert': 1.2,\n",
              " 'alienation': -1.1,\n",
              " 'alive': 1.6,\n",
              " 'allergic': -1.2,\n",
              " 'allow': 0.9,\n",
              " 'alone': -1.0,\n",
              " 'alright': 1.0,\n",
              " 'amaze': 2.5,\n",
              " 'amazed': 2.2,\n",
              " 'amazedly': 2.1,\n",
              " 'amazement': 2.5,\n",
              " 'amazements': 2.2,\n",
              " 'amazes': 2.2,\n",
              " 'amazing': 2.8,\n",
              " 'amazon': 0.7,\n",
              " 'amazonite': 0.2,\n",
              " 'amazons': -0.1,\n",
              " 'amazonstone': 1.0,\n",
              " 'amazonstones': 0.2,\n",
              " 'ambitious': 2.1,\n",
              " 'ambivalent': 0.5,\n",
              " 'amor': 3.0,\n",
              " 'amoral': -1.6,\n",
              " 'amoralism': -0.7,\n",
              " 'amoralisms': -0.7,\n",
              " 'amoralities': -1.2,\n",
              " 'amorality': -1.5,\n",
              " 'amorally': -1.0,\n",
              " 'amoretti': 0.2,\n",
              " 'amoretto': 0.6,\n",
              " 'amorettos': 0.3,\n",
              " 'amorino': 1.2,\n",
              " 'amorist': 1.6,\n",
              " 'amoristic': 1.0,\n",
              " 'amorists': 0.1,\n",
              " 'amoroso': 2.3,\n",
              " 'amorous': 1.8,\n",
              " 'amorously': 2.3,\n",
              " 'amorousness': 2.0,\n",
              " 'amorphous': -0.2,\n",
              " 'amorphously': 0.1,\n",
              " 'amorphousness': 0.3,\n",
              " 'amort': -2.1,\n",
              " 'amortise': 0.5,\n",
              " 'amortised': -0.2,\n",
              " 'amortises': 0.1,\n",
              " 'amortizable': 0.5,\n",
              " 'amortization': 0.6,\n",
              " 'amortizations': 0.2,\n",
              " 'amortize': -0.1,\n",
              " 'amortized': 0.8,\n",
              " 'amortizes': 0.6,\n",
              " 'amortizing': 0.8,\n",
              " 'amusable': 0.7,\n",
              " 'amuse': 1.7,\n",
              " 'amused': 1.8,\n",
              " 'amusedly': 2.2,\n",
              " 'amusement': 1.5,\n",
              " 'amusements': 1.5,\n",
              " 'amuser': 1.1,\n",
              " 'amusers': 1.3,\n",
              " 'amuses': 1.7,\n",
              " 'amusia': 0.3,\n",
              " 'amusias': -0.4,\n",
              " 'amusing': 1.6,\n",
              " 'amusingly': 0.8,\n",
              " 'amusingness': 1.8,\n",
              " 'amusive': 1.7,\n",
              " 'anger': -2.7,\n",
              " 'angered': -2.3,\n",
              " 'angering': -2.2,\n",
              " 'angerly': -1.9,\n",
              " 'angers': -2.3,\n",
              " 'angrier': -2.3,\n",
              " 'angriest': -3.1,\n",
              " 'angrily': -1.8,\n",
              " 'angriness': -1.7,\n",
              " 'angry': -2.3,\n",
              " 'anguish': -2.9,\n",
              " 'anguished': -1.8,\n",
              " 'anguishes': -2.1,\n",
              " 'anguishing': -2.7,\n",
              " 'animosity': -1.9,\n",
              " 'annoy': -1.9,\n",
              " 'annoyance': -1.3,\n",
              " 'annoyances': -1.8,\n",
              " 'annoyed': -1.6,\n",
              " 'annoyer': -2.2,\n",
              " 'annoyers': -1.5,\n",
              " 'annoying': -1.7,\n",
              " 'annoys': -1.8,\n",
              " 'antagonism': -1.9,\n",
              " 'antagonisms': -1.2,\n",
              " 'antagonist': -1.9,\n",
              " 'antagonistic': -1.7,\n",
              " 'antagonistically': -2.2,\n",
              " 'antagonists': -1.7,\n",
              " 'antagonize': -2.0,\n",
              " 'antagonized': -1.4,\n",
              " 'antagonizes': -0.5,\n",
              " 'antagonizing': -2.7,\n",
              " 'anti': -1.3,\n",
              " 'anticipation': 0.4,\n",
              " 'anxieties': -0.6,\n",
              " 'anxiety': -0.7,\n",
              " 'anxious': -1.0,\n",
              " 'anxiously': -0.9,\n",
              " 'anxiousness': -1.0,\n",
              " 'aok': 2.0,\n",
              " 'apathetic': -1.2,\n",
              " 'apathetically': -0.4,\n",
              " 'apathies': -0.6,\n",
              " 'apathy': -1.2,\n",
              " 'apeshit': -0.9,\n",
              " 'apocalyptic': -3.4,\n",
              " 'apologise': 1.6,\n",
              " 'apologised': 0.4,\n",
              " 'apologises': 0.8,\n",
              " 'apologising': 0.2,\n",
              " 'apologize': 0.4,\n",
              " 'apologized': 1.3,\n",
              " 'apologizes': 1.5,\n",
              " 'apologizing': -0.3,\n",
              " 'apology': 0.2,\n",
              " 'appall': -2.4,\n",
              " 'appalled': -2.0,\n",
              " 'appalling': -1.5,\n",
              " 'appallingly': -2.0,\n",
              " 'appalls': -1.9,\n",
              " 'appease': 1.1,\n",
              " 'appeased': 0.9,\n",
              " 'appeases': 0.9,\n",
              " 'appeasing': 1.0,\n",
              " 'applaud': 2.0,\n",
              " 'applauded': 1.5,\n",
              " 'applauding': 2.1,\n",
              " 'applauds': 1.4,\n",
              " 'applause': 1.8,\n",
              " 'appreciate': 1.7,\n",
              " 'appreciated': 2.3,\n",
              " 'appreciates': 2.3,\n",
              " 'appreciating': 1.9,\n",
              " 'appreciation': 2.3,\n",
              " 'appreciations': 1.7,\n",
              " 'appreciative': 2.6,\n",
              " 'appreciatively': 1.8,\n",
              " 'appreciativeness': 1.6,\n",
              " 'appreciator': 2.6,\n",
              " 'appreciators': 1.5,\n",
              " 'appreciatory': 1.7,\n",
              " 'apprehensible': 1.1,\n",
              " 'apprehensibly': -0.2,\n",
              " 'apprehension': -2.1,\n",
              " 'apprehensions': -0.9,\n",
              " 'apprehensively': -0.3,\n",
              " 'apprehensiveness': -0.7,\n",
              " 'approval': 2.1,\n",
              " 'approved': 1.8,\n",
              " 'approves': 1.7,\n",
              " 'ardent': 2.1,\n",
              " 'arguable': -1.0,\n",
              " 'arguably': -1.0,\n",
              " 'argue': -1.4,\n",
              " 'argued': -1.5,\n",
              " 'arguer': -1.6,\n",
              " 'arguers': -1.4,\n",
              " 'argues': -1.6,\n",
              " 'arguing': -2.0,\n",
              " 'argument': -1.5,\n",
              " 'argumentative': -1.5,\n",
              " 'argumentatively': -1.8,\n",
              " 'argumentive': -1.5,\n",
              " 'arguments': -1.7,\n",
              " 'arrest': -1.4,\n",
              " 'arrested': -2.1,\n",
              " 'arrests': -1.9,\n",
              " 'arrogance': -2.4,\n",
              " 'arrogances': -1.9,\n",
              " 'arrogant': -2.2,\n",
              " 'arrogantly': -1.8,\n",
              " 'ashamed': -2.1,\n",
              " 'ashamedly': -1.7,\n",
              " 'ass': -2.5,\n",
              " 'assassination': -2.9,\n",
              " 'assassinations': -2.7,\n",
              " 'assault': -2.8,\n",
              " 'assaulted': -2.4,\n",
              " 'assaulting': -2.3,\n",
              " 'assaultive': -2.8,\n",
              " 'assaults': -2.5,\n",
              " 'asset': 1.5,\n",
              " 'assets': 0.7,\n",
              " 'assfucking': -2.5,\n",
              " 'assholes': -2.8,\n",
              " 'assurance': 1.4,\n",
              " 'assurances': 1.4,\n",
              " 'assure': 1.4,\n",
              " 'assured': 1.5,\n",
              " 'assuredly': 1.6,\n",
              " 'assuredness': 1.4,\n",
              " 'assurer': 0.9,\n",
              " 'assurers': 1.1,\n",
              " 'assures': 1.3,\n",
              " 'assurgent': 1.3,\n",
              " 'assuring': 1.6,\n",
              " 'assuror': 0.5,\n",
              " 'assurors': 0.7,\n",
              " 'astonished': 1.6,\n",
              " 'astound': 1.7,\n",
              " 'astounded': 1.8,\n",
              " 'astounding': 1.8,\n",
              " 'astoundingly': 2.1,\n",
              " 'astounds': 2.1,\n",
              " 'attachment': 1.2,\n",
              " 'attachments': 1.1,\n",
              " 'attack': -2.1,\n",
              " 'attacked': -2.0,\n",
              " 'attacker': -2.7,\n",
              " 'attackers': -2.7,\n",
              " 'attacking': -2.0,\n",
              " 'attacks': -1.9,\n",
              " 'attract': 1.5,\n",
              " 'attractancy': 0.9,\n",
              " 'attractant': 1.3,\n",
              " 'attractants': 1.4,\n",
              " 'attracted': 1.8,\n",
              " 'attracting': 2.1,\n",
              " 'attraction': 2.0,\n",
              " 'attractions': 1.8,\n",
              " 'attractive': 1.9,\n",
              " 'attractively': 2.2,\n",
              " 'attractiveness': 1.8,\n",
              " 'attractivenesses': 2.1,\n",
              " 'attractor': 1.2,\n",
              " 'attractors': 1.2,\n",
              " 'attracts': 1.7,\n",
              " 'audacious': 0.9,\n",
              " 'authority': 0.3,\n",
              " 'aversion': -1.9,\n",
              " 'aversions': -1.1,\n",
              " 'aversive': -1.6,\n",
              " 'aversively': -0.8,\n",
              " 'avert': -0.7,\n",
              " 'averted': -0.3,\n",
              " 'averts': -0.4,\n",
              " 'avid': 1.2,\n",
              " 'avoid': -1.2,\n",
              " 'avoidance': -1.7,\n",
              " 'avoidances': -1.1,\n",
              " 'avoided': -1.4,\n",
              " 'avoider': -1.8,\n",
              " 'avoiders': -1.4,\n",
              " 'avoiding': -1.4,\n",
              " 'avoids': -0.7,\n",
              " 'await': 0.4,\n",
              " 'awaited': -0.1,\n",
              " 'awaits': 0.3,\n",
              " 'award': 2.5,\n",
              " 'awardable': 2.4,\n",
              " 'awarded': 1.7,\n",
              " 'awardee': 1.8,\n",
              " 'awardees': 1.2,\n",
              " 'awarder': 0.9,\n",
              " 'awarders': 1.3,\n",
              " 'awarding': 1.9,\n",
              " 'awards': 2.0,\n",
              " 'awesome': 3.1,\n",
              " 'awful': -2.0,\n",
              " 'awkward': -0.6,\n",
              " 'awkwardly': -1.3,\n",
              " 'awkwardness': -0.7,\n",
              " 'axe': -0.4,\n",
              " 'axed': -1.3,\n",
              " 'backed': 0.1,\n",
              " 'backing': 0.1,\n",
              " 'backs': -0.2,\n",
              " 'bad': -2.5,\n",
              " 'badass': 1.4,\n",
              " 'badly': -2.1,\n",
              " 'bailout': -0.4,\n",
              " 'bamboozle': -1.5,\n",
              " 'bamboozled': -1.5,\n",
              " 'bamboozles': -1.5,\n",
              " 'ban': -2.6,\n",
              " 'banish': -1.9,\n",
              " 'bankrupt': -2.6,\n",
              " 'bankster': -2.1,\n",
              " 'banned': -2.0,\n",
              " 'bargain': 0.8,\n",
              " 'barrier': -0.5,\n",
              " 'bashful': -0.1,\n",
              " 'bashfully': 0.2,\n",
              " 'bashfulness': -0.8,\n",
              " 'bastard': -2.5,\n",
              " 'bastardies': -1.8,\n",
              " 'bastardise': -2.1,\n",
              " 'bastardised': -2.3,\n",
              " 'bastardises': -2.3,\n",
              " 'bastardising': -2.6,\n",
              " 'bastardization': -2.4,\n",
              " 'bastardizations': -2.1,\n",
              " 'bastardize': -2.4,\n",
              " 'bastardized': -2.0,\n",
              " 'bastardizes': -1.8,\n",
              " 'bastardizing': -2.3,\n",
              " 'bastardly': -2.7,\n",
              " 'bastards': -3.0,\n",
              " 'bastardy': -2.7,\n",
              " 'battle': -1.6,\n",
              " 'battled': -1.2,\n",
              " 'battlefield': -1.6,\n",
              " 'battlefields': -0.9,\n",
              " 'battlefront': -1.2,\n",
              " 'battlefronts': -0.8,\n",
              " 'battleground': -1.7,\n",
              " 'battlegrounds': -0.6,\n",
              " 'battlement': -0.4,\n",
              " 'battlements': -0.4,\n",
              " 'battler': -0.8,\n",
              " 'battlers': -0.2,\n",
              " 'battles': -1.6,\n",
              " 'battleship': -0.1,\n",
              " 'battleships': -0.5,\n",
              " 'battlewagon': -0.3,\n",
              " 'battlewagons': -0.5,\n",
              " 'battling': -1.1,\n",
              " 'beaten': -1.8,\n",
              " 'beatific': 1.8,\n",
              " 'beating': -2.0,\n",
              " 'beaut': 1.6,\n",
              " 'beauteous': 2.5,\n",
              " 'beauteously': 2.6,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sa.polarity_scores(text=':)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8maWReXVknD",
        "outputId": "4f621a6f-49f1-4fa6-b74a-06d4e9c30a42"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4588}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VADER에 정의된 7,500개의 토큰 중 빈칸이 포함된 것은 3개 뿐이고, 그 3 중 실제로 n-gram인 것은 둘 뿐이다.\n",
        "[(tok, score) for tok, score in sa.lexicon.items() if \" \" in tok]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWbPwBE94y1a",
        "outputId": "00e36d59-edda-45e5-b37e-f95475418880"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"( '}{' )\", 1.6),\n",
              " (\"can't stand\", -2.0),\n",
              " ('fed up', -1.8),\n",
              " ('screwed up', -1.5)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sa.polarity_scores(text=\"Python is a very readable and it's great for NLP.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F-cDRAm56fA",
        "outputId": "5bd2edaf-4f65-4e88-e49c-15a7b6e8c144"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.687, 'pos': 0.313, 'compound': 0.6249}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sa.polarity_scores(text=\"Python is not a bad choice for most applications.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRe_dhDE7AZ4",
        "outputId": "42b5e506-291a-4438-834f-7cdae72455da"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.737, 'pos': 0.263, 'compound': 0.431}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VADER 규칙 기반 감정 분석기\n",
        "# 장점: 일반적인 형태의 문장들에서 점수를 잘 도출함을 확인할 수 있음.\n",
        "# 단점: 7500개의 토큰들만에 대해 점수를 매기므로 새로운 토큰들이 추가되면 프로그램 자체를 패치해줘야 한다.\n",
        "corpus = [\"Absolutely Perfect! Love it! :-) :-) :-)\",\n",
        "          \"Horrible! Completely useless :(\",\n",
        "          \"It was OK. Some good and some bad things.\"]\n",
        "\n",
        "for doc in corpus:\n",
        "  scores = sa.polarity_scores(doc)\n",
        "  print(f\"{scores['compound']}: {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eod2MX428YPb",
        "outputId": "1ede6434-5895-4fdd-ce59-3255a1350514"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9428: Absolutely Perfect! Love it! :-) :-) :-)\n",
            "-0.8768: Horrible! Completely useless :(\n",
            "-0.1531: It was OK. Some good and some bad things.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}